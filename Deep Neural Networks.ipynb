{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66744478-3bd8-4ac4-bc76-70fb5cd309b4",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks on a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a38224-d27e-40e7-b961-04c88bf88fad",
   "metadata": {},
   "source": [
    "## Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b724ab3-e137-4478-8a78-112c168df95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068772ff-3f34-4161-9700-0e2acb287e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='./data',\n",
    "               download=False,\n",
    "               transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1b1614-731e-4422-a03b-756ea7c5df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3b3af80f70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894ca205-6792-4450-9e4a-05489b3a63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUnction that randomly picks a given fraction of the images for the validation set\n",
    "def split_indices(n, val_pct):\n",
    "    #Determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "    #Create random permutation of 0 to n-1\n",
    "    idxs = np.random. permutation(n)\n",
    "    #Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b954b1cd-57ad-4e5e-b59d-ec03f13813af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n",
      "Sample val indices  [25767 36004 55884 58577  9580 55420 38704 19148  1793 21477 14697 58148\n",
      " 46172  6587 12414 43943 21524   209 28291 46502]\n"
     ]
    }
   ],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset),val_pct=0.2)\n",
    "print(len(train_indices),len(val_indices))\n",
    "print('Sample val indices ', val_indices[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320ab21e-c1ea-4c1d-b9ba-2cf331066abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "#Training sampler and data loader\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_dl = DataLoader(dataset,\n",
    "                          batch_size,\n",
    "                          sampler=train_sampler\n",
    ")\n",
    "\n",
    "#Validation sampler and data loader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "valid_dl = DataLoader(dataset,\n",
    "                       batch_size,\n",
    "                       sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11f961-5953-4129-b872-24583a53c0c7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9465221a-a226-409d-81a6-fdc8ba9de6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "#Logistic regression model\n",
    "model = nn.Linear(input_size,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d162bb5-0ff2-4efe-ad5c-3c13a77a3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24f3184d-caab-42ee-8851-689827a3c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    \"\"\"Feedfoward neuaral network with 1 hidden layer\"\"\"\n",
    "    def __init__(self, in_size, hidden_size,out_size):\n",
    "        super().__init__()\n",
    "        #hidden layer\n",
    "        self.linear1 = nn.Linear(in_size,hidden_size)\n",
    "        #output layer\n",
    "        self.linear2 = nn.Linear(hidden_size,out_size)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        #Flatten the image tensors\n",
    "        xb = xb.view(xb.size(0),-1)\n",
    "        #Get the intermediate outputs using hidden layer\n",
    "        out = self.linear1(xb)\n",
    "        #Apply the activation function\n",
    "        out = F.relu(out)\n",
    "        #Get predictions using output layer\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba2ce39-f958-4e8e-b9fd-e1109e67c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "\n",
    "model = MnistModel(input_size,hidden_size=32,\n",
    "                   out_size=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c18d4a-6bb5-4c8b-ac34-e6fbad488360",
   "metadata": {},
   "source": [
    "Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f8ea0a-7ccc-427c-981a-7d8f324d9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784])\n",
      "torch.Size([32])\n",
      "torch.Size([10, 32])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for t in model.parameters():\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037c7bc8-080f-4981-bcf2-168aa60bea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images.shape torch.Size([100, 1, 28, 28])\n",
      "Loss:  2.315122604370117\n",
      "Output.shape:  torch.Size([100, 10])\n",
      "Sample Outputs:\n",
      " tensor([[-0.2257,  0.0243,  0.1996, -0.0011, -0.1962,  0.0633,  0.0777,  0.0182,\n",
      "          0.0674, -0.1221],\n",
      "        [-0.1416,  0.0507,  0.2123, -0.0005, -0.1583,  0.0306,  0.1326, -0.0127,\n",
      "         -0.0465, -0.1447]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    print('Images.shape',images.shape)\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs,labels)\n",
    "    print('Loss: ', loss.item())\n",
    "    break\n",
    "\n",
    "print('Output.shape: ', outputs.shape)\n",
    "print('Sample Outputs:\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad967433-eb11-40d0-b136-424cc8ba257f",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5517780-d19a-465b-aa84-24ecd7c6c94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "098279b7-6060-4f53-8d00-cb40b8a22a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecdb727b-a918-43e6-82b1-fe34fde57e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fcadf-5053-42be-a80d-0eb5a157d8fb",
   "metadata": {},
   "source": [
    "Function that can move data and model to a chosen device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cd3e333-b86a-43d1-bbe7-fd3f91d98acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x,device) for x in data]\n",
    "    return data.to(device,non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6fca748-14f0-47dd-862e-b3f2e16cffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 3, 4]), tensor([1, 2, 3, 4])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_device([torch.tensor([1,2,3,4]),torch.tensor([1,2,3,4])],device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dd54d9e-7313-4c51-9a4d-6e03beb2cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    print(images.shape)\n",
    "    images = to_device(images, device)\n",
    "    print(images.device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95fce342-2335-42da-9c73-f05bff5cda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dataloader,device):\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to a device\"\"\"\n",
    "        for batch  in self.dataloader:\n",
    "            yield to_device(batch,self.device)\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0eeadddd-4763-4f38-8d8a-703f1da11921",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl,device)\n",
    "valid_dl = DeviceDataLoader(valid_dl,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f55e3543-ab5d-4cb7-9f23-23bc48e750f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.device: cpu\n",
      "yb: tensor([0, 3, 5, 4, 5, 7, 4, 7, 3, 9, 9, 8, 9, 2, 9, 5, 3, 4, 0, 0, 4, 1, 1, 7,\n",
      "        7, 0, 7, 3, 3, 9, 4, 1, 9, 8, 5, 9, 2, 6, 1, 5, 1, 6, 9, 9, 6, 2, 1, 3,\n",
      "        8, 5, 3, 3, 8, 0, 1, 3, 7, 2, 2, 6, 8, 8, 8, 9, 3, 9, 2, 9, 0, 4, 8, 3,\n",
      "        9, 0, 6, 8, 4, 6, 8, 1, 3, 6, 0, 8, 5, 4, 1, 4, 3, 4, 1, 1, 2, 2, 1, 5,\n",
      "        1, 1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in valid_dl:\n",
    "    print('xb.device:',xb.device)\n",
    "    print('yb:',yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f719f30-c3d1-462f-80d9-b2a4d4a1ff33",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6063eb3-bed3-4f8d-bfb6-c16a32f7c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
    "    #Calculate loss\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds,yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        #Compute gradients\n",
    "        loss.backward()\n",
    "        #Update parameters\n",
    "        opt.step()\n",
    "        #Reset gradients\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        #Compute the metric\n",
    "        metric_result = metric(preds,yb)\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8b8a53-bdfe-4100-a01a-e119d96ce0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "        #Pass each batch throuh the model\n",
    "        \n",
    "        results = [loss_batch(model,loss_fn,xb,yb,metric=metric) for xb,yb in valid_dl]\n",
    "        #Separate losses, count and metrics\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        #Total size of the dataset\n",
    "        total = np.sum(nums)\n",
    "        #Avg. loss across batches\n",
    "        avg_loss = np.sum(np.multiply(losses,nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            #Avg. of metric across batches\n",
    "            avg_metric = np.sum(np.multiply(metrics,nums)) / total\n",
    "        return avg_loss, total, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4f7231-58a2-460f-8ed4-c65adb9fde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs,lr,model,loss_fn,train_dl,\n",
    "        valid_dl,metric=None, opt_fn=None):\n",
    "    losses,metrics = [],[]\n",
    "    \n",
    "    #Instantiate the optimizer\n",
    "    if opt_fn is None: opt_fn = torch.optim.SGD\n",
    "    opt = opt_fn(model.parameters(),lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #Training\n",
    "        for xb,yb in train_dl:\n",
    "            loss_batch(model,loss_fn,xb,yb,opt)\n",
    "            \n",
    "        #Evaluation\n",
    "        result = evaluate(model,loss_fn,valid_dl,metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        #Record the loss & metric\n",
    "        losses.append(val_loss)\n",
    "        metrics.append(val_metric)\n",
    "        \n",
    "        #Print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                 .format(epoch+1,epochs,val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                 .format(epoch+1,epochs,val_loss,metric.__name__,val_metric))\n",
    "    \n",
    "    return losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f81c8c7a-cde5-4d57-93bd-ba93107089b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs,labels):\n",
    "    _, preds =torch.max(outputs,dim=1)\n",
    "    return torch.sum(preds==labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "81732c29-31bc-4916-9614-630d1730fc3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (linear1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (linear2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model on GPU\n",
    "model = MnistModel(input_size,hidden_size=32,out_size=num_classes)\n",
    "to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a0f30ab-776b-43ce-8106-d1362e66afe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3093, Accuracy:.0.0943\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, F.cross_entropy,\n",
    "                                   valid_dl, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy:.{:.4f}'.format(val_loss,val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da1a5d09-22de-4afb-a51d-932664cf128b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2109, accuracy: 0.9378\n",
      "Epoch [2/5], Loss: 0.1622, accuracy: 0.9532\n",
      "Epoch [3/5], Loss: 0.1530, accuracy: 0.9533\n",
      "Epoch [4/5], Loss: 0.1455, accuracy: 0.9564\n",
      "Epoch [5/5], Loss: 0.1318, accuracy: 0.9603\n"
     ]
    }
   ],
   "source": [
    "losses1, metrics1 = fit(epochs=5,lr=0.5,model=model,loss_fn=F.cross_entropy,\n",
    "                        train_dl=train_dl,valid_dl=valid_dl,metric=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f42f5-53e0-43aa-a195-49e9b136425e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1125, accuracy: 0.9672\n",
      "Epoch [2/5], Loss: 0.1122, accuracy: 0.9674\n",
      "Epoch [3/5], Loss: 0.1114, accuracy: 0.9677\n",
      "Epoch [4/5], Loss: 0.1120, accuracy: 0.9677\n"
     ]
    }
   ],
   "source": [
    "losses2, metrics2 = fit(epochs=5,lr=0.1,model=model,loss_fn=F.cross_entropy,\n",
    "                        train_dl=train_dl,valid_dl=valid_dl,metric=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d489dd1-dfa0-43b0-a9d7-b1f196a6ad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c603ff-87da-4f63-aad8-63d737ad5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = [val_acc]+ metrics1+metrics2\n",
    "plt.plot(accuracies,'-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs No of epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a7730-73f4-4455-b68b-ccae3c7c2c12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8deeee6-656f-4a5a-a5a1-e6480bcb6ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538b92c-c1fb-4131-888c-0d6a7c6f5b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc95ef19-a04f-4762-a3fb-8a535dbc0cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb83de0-ffb3-4eb7-b152-0f11f50ffe29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fe448-d7d7-49ed-95ce-71d17e028adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5b0c9-010b-4200-9516-34728f7e545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159c3fb-822e-4daa-8a71-bd61a0300da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13791b4-ce86-4071-9c02-60404eede642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a764f-0908-4875-a1df-adb86acd0107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60872be1-47f1-40bf-8689-ecf249681ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947ea9b-4b09-4275-b022-5aefaea1e4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2f8cb-5925-4ecc-b539-9acedbddc690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614d060-81fc-46a0-bd37-52748e818897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
